{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBO-7StHAUJY",
        "outputId": "ebe31984-075d-4ba9-a8c4-0461f1fb7dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient at x = 2: 20\n",
            "Gradient using Autograd at x = 2: tensor(20.)\n",
            "Jacobian Matrix:\n",
            " tensor([[4., 1.],\n",
            "        [3., 2.]])\n",
            "Gradient before detach: tensor(218.3926)\n",
            "Value after detach (no gradient tracking): tensor(3.)\n",
            "First Derivative: tensor(41., grad_fn=<AddBackward0>)\n",
            "Second Derivative: tensor(52., grad_fn=<AddBackward0>)\n",
            "Third Derivative: tensor(48., grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Exercise 1: Verify Gradients for a Polynomial\n",
        "# Function: f(x) = 3x^3 - 4x^2 + 7\n",
        "# Derivative: f'(x) = 9x^2 - 8x\n",
        "x_manual = 2\n",
        "f_prime_manual = 9 * x_manual**2 - 8 * x_manual\n",
        "print(\"Manual Gradient at x = 2:\", f_prime_manual)\n",
        "\n",
        "# Create a tensor with requires_grad=True\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# Define the function\n",
        "f = 3 * x**3 - 4 * x**2 + 7\n",
        "\n",
        "# Compute the gradient\n",
        "f.backward()\n",
        "\n",
        "# Access the gradient\n",
        "print(\"Gradient using Autograd at x = 2:\", x.grad)\n",
        "\n",
        "# Exercise 2: Jacobian Matrix for Multivariable Function\n",
        "# Function: f(x1, x2) = [x1^2 + x2, x1 * x2]\n",
        "# Compute Jacobian using autograd\n",
        "\n",
        "def compute_jacobian():\n",
        "    # Define input tensor\n",
        "    x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "    # Define the function\n",
        "    f1 = x[0]**2 + x[1]\n",
        "    f2 = x[0] * x[1]\n",
        "    f = torch.stack([f1, f2])\n",
        "\n",
        "    # Compute Jacobian\n",
        "    jacobian = []\n",
        "    for i in range(f.size(0)):\n",
        "        f[i].backward(retain_graph=True)\n",
        "        jacobian.append(x.grad.clone())\n",
        "        x.grad.zero_()\n",
        "\n",
        "    jacobian = torch.stack(jacobian)\n",
        "    return jacobian\n",
        "\n",
        "jacobian = compute_jacobian()\n",
        "print(\"Jacobian Matrix:\\n\", jacobian)\n",
        "\n",
        "# Exercise 3: Detach and Gradient Checking\n",
        "# Function: g(x) = exp(x^2)\n",
        "\n",
        "def gradient_detach():\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    # Define the function\n",
        "    g = torch.exp(x**2)\n",
        "\n",
        "    # Compute the gradient\n",
        "    g.backward()\n",
        "    print(\"Gradient before detach:\", x.grad)\n",
        "\n",
        "    # Detach x and modify it\n",
        "    x_detached = x.detach()\n",
        "    x_detached += 1\n",
        "\n",
        "    print(\"Value after detach (no gradient tracking):\", x_detached)\n",
        "\n",
        "gradient_detach()\n",
        "\n",
        "# Exercise 4: Higher-Order Gradients\n",
        "# Function: h(x) = x^4 + 2x^2 + x\n",
        "\n",
        "def higher_order_gradients():\n",
        "    x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "    # Define the function\n",
        "    h = x**4 + 2*x**2 + x\n",
        "\n",
        "    # Compute the first derivative\n",
        "    first_derivative = torch.autograd.grad(h, x, create_graph=True)[0]\n",
        "    print(\"First Derivative:\", first_derivative)\n",
        "\n",
        "    # Compute the second derivative\n",
        "    second_derivative = torch.autograd.grad(first_derivative, x, create_graph=True)[0]\n",
        "    print(\"Second Derivative:\", second_derivative)\n",
        "\n",
        "    # Compute the third derivative\n",
        "    third_derivative = torch.autograd.grad(second_derivative, x, create_graph=True)[0]\n",
        "    print(\"Third Derivative:\", third_derivative)\n",
        "\n",
        "higher_order_gradients()"
      ]
    }
  ]
}