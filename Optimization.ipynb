{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWCMhFM3-7Se",
        "outputId": "116493d4-a64d-444c-9940-6b78efbc7e75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: x = -0.30000001192092896, Loss = 2.0\n",
            "Step 2: x = -0.5400000214576721, Loss = 1.1899999380111694\n",
            "Step 3: x = -0.7319999933242798, Loss = 0.6715998649597168\n",
            "Step 4: x = -0.8855999708175659, Loss = 0.33982396125793457\n",
            "Step 5: x = -1.0084799528121948, Loss = 0.12748754024505615\n",
            "Step 6: x = -1.1067839860916138, Loss = -0.008408069610595703\n",
            "Step 7: x = -1.185427188873291, Loss = -0.09538125991821289\n",
            "Step 8: x = -1.2483417987823486, Loss = -0.15104389190673828\n",
            "Step 9: x = -1.298673391342163, Loss = -0.18666815757751465\n",
            "Step 10: x = -1.3389387130737305, Loss = -0.20946764945983887\n",
            "Step 11: x = -1.3711509704589844, Loss = -0.2240591049194336\n",
            "Step 12: x = -1.3969208002090454, Loss = -0.2333979606628418\n",
            "Step 13: x = -1.4175366163253784, Loss = -0.23937463760375977\n",
            "Step 14: x = -1.4340293407440186, Loss = -0.24319958686828613\n",
            "Step 15: x = -1.447223424911499, Loss = -0.24564766883850098\n",
            "Step 1: x = 1.0, Loss = 25.0\n",
            "Step 2: x = 1.7999999523162842, Loss = 16.0\n",
            "Step 3: x = 2.440000057220459, Loss = 10.24000072479248\n",
            "Step 4: x = 2.952000141143799, Loss = 6.553599834442139\n",
            "Step 5: x = 3.361600160598755, Loss = 4.194303512573242\n",
            "Step 6: x = 3.6892800331115723, Loss = 2.684354066848755\n",
            "Step 7: x = 3.9514241218566895, Loss = 1.717986822128296\n",
            "Step 8: x = 4.161139488220215, Loss = 1.0995113849639893\n",
            "Step 9: x = 4.328911781311035, Loss = 0.7036869525909424\n",
            "Step 10: x = 4.46312952041626, Loss = 0.45035940408706665\n",
            "Step 11: x = 4.5705037117004395, Loss = 0.28822991251945496\n",
            "Step 12: x = 4.656403064727783, Loss = 0.18446706235408783\n",
            "Step 13: x = 4.725122451782227, Loss = 0.11805885285139084\n",
            "Step 14: x = 4.780097961425781, Loss = 0.0755576640367508\n",
            "Step 15: x = 4.824078559875488, Loss = 0.048356905579566956\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: Optimize a Quadratic Function\n",
        "# Function: f(x) = x^2 + 3x + 2\n",
        "# Goal: Minimize the function using gradient descent\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define the quadratic function\n",
        "def quadratic(x):\n",
        "    return x**2 + 3 * x + 2\n",
        "\n",
        "# Optimization for the quadratic function\n",
        "x = torch.tensor([0.0], requires_grad=True)\n",
        "optimizer = torch.optim.SGD([x], lr=0.1)\n",
        "\n",
        "for step in range(15):\n",
        "    optimizer.zero_grad()\n",
        "    loss = quadratic(x)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step+1}: x = {x.item()}, Loss = {loss.item()}\")\n",
        "\n",
        "# Exercise 2: Optimize a Distance Function\n",
        "# Function: f(x) = (x - y)^2\n",
        "# Goal: Minimize the distance between x and y\n",
        "\n",
        "y = torch.tensor([5.0])\n",
        "x = torch.tensor([0.0], requires_grad=True)\n",
        "optimizer = torch.optim.SGD([x], lr=0.1)\n",
        "\n",
        "for step in range(15):\n",
        "    optimizer.zero_grad()\n",
        "    loss = (x - y).pow(2).sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step+1}: x = {x.item()}, Loss = {loss.item()}\")"
      ]
    }
  ]
}